\chapter{Experiments}

\section{Methods}

It is difficult to directly measure cryptographic parameters ($< 2^{-128}$). The common method is to compute DFR's for smaller code sizes then extrapolate in order to estimate the DFR for larger parameters \cite{SV19,DGK20a,DGK20b}. In this work, we analyse the decoding behaviour for BIKE parameters at the $\lambda = 20$ bits of security. 

The parameters were selected according to BIKE design principles and the error weight $t$ is reduced to prevent almost likely decoding failures. Initial selected parameters are as follows: $(r,2d,t,\lambda) = (523,30,18,20)$. Later we include $389 \le r \le 827$ for prime $r$ such that $x^r - 1$ has only two irreducible factors modulo $2$.

We use the Black-Grey-Flip (BGF) decoder in all experiments. Within the decoder, we used the original threshold selection function, defined as the maximum of two values: $\frac{d+1}{2}$, and the output of an adaptive threshold function, defined in section 2.5.1 of the BIKE v1.0 specification \cite{BIKE2017}. The affine threshold functions in the current version of BIKE are derived from this original threshold rule.

We implement the weak key rejection algorithm defined in \cite[Algorithm 15.3]{Vasseur-thesis} in order to filter out the weak keys that impedes decoding. The definition of weak key depends on a parameter $T$. For cryptographic size parameters ($\lambda \geq 128$), Vasseur sets $T=10$ for BIKE parameters. We instead use $T = 3$ for the weak key threshold, the smallest value of $T$ for which finding non-weak keys is feasible. This is justified by the following empirical observation: If we set $T = 4$, the decoding failure rate increases enormously; for example, an experiment with $(r, T) = (587, 4)$ observed a DFR on the order of $2^{-8}$, compared to around $2^{-20}$ for $(r, T) = (587, 3)$.

%\begin{center}
%\begin{tabular}{ |c|c|c|c| } 
%\hline
%r & w & t & $\lambda$\\
%\hline
%523 & 30 & 18 & 20 \\
%\hline
%\end{tabular}
%\end{center}

We use the Boston University Shared Computing Cluster \cite{SCC}, a heterogeneous Linux-based computing cluster with approximately 21000 cores, to run \SageMath{} and Rust implementations of the BGF decoder \cite{BIKE,DGK20a} in all experiments. The experiments yielded a graph with both the waterfall and error floor regions for our parameter set in addition to many explicit examples of decoding failures that can be used for future analysis. All raw data and the decoder used for this paper are available at \cite{raw-data-decoder}.

We first compute an average DFR for all suitable block lengths $r$ as follows.  For $r$ in Table \ref{table:DFR}, we sample a random key $H$, rejecting any weak keys of types I,II,III as defined before, a random vector $e \in \FF_2^{2r}$ of weight $t$, compute $s=He^T$, run BGF decoder on input $(H, s)$, and record the total number of failures.  This procedure is run $N$ times where $10^8 \le 10^9$ to ensure there are enough decoding failures at each $r$ for robust statistical analysis.
The error vectors tested in the DFR experiment all had weight $18$. The results of this experiment are displayed in Table \ref{table:DFR} and plotted with best fit curves in Figure \ref{fig:DFR}.

\begin{remark}
Initially, the experiments in \SageMath{} used different numbers of trials ranging from $10^3$ to $10^8$ for different block sizes $r$. This is to ensure that we have enough data to compute the average DFR. Smaller block sizes have more decoding failures whereas larger block sizes have much fewer, calling the need for this adjustment. A future Rust implementation significantly optimised this process. Comparing the results from both implementations, there were quantitative differences but no qualitative ones.
\end{remark}

